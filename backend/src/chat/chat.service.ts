// backend/src/chat/chat.service.ts
import { Injectable, Logger, BadRequestException } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { PrismaService } from '../prisma/prisma.service';
import { VectorService } from '../vector/vector.service';
import { ChatRequestDto } from './dto/chat-request.dto';
import { ChatResponseDto } from './dto/chat-response.dto';
import OpenAI from 'openai';

@Injectable()
export class ChatService {
  private readonly logger = new Logger(ChatService.name);
  private openai: OpenAI;

  constructor(
    private prisma: PrismaService,
    private vectorService: VectorService,
    private configService: ConfigService,
  ) {
    this.openai = new OpenAI({
      apiKey: this.configService.get<string>('OPENAI_API_KEY'),
    });
  }

  /**
   * å¤„ç†èŠå¤©æ¶ˆæ¯
   */
  async processMessage(chatRequest: ChatRequestDto): Promise<ChatResponseDto> {
    try {
      const { message, conversationId } = chatRequest;

      // 1. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£å—
      this.logger.log(`ğŸ” Searching for relevant content for: "${message}"`);
      const relevantChunks = await this.vectorService.searchSimilarChunks(message, 5);

      if (relevantChunks.length === 0) {
        return {
          answer: 'æŠ±æ­‰ï¼Œæˆ‘åœ¨æ‚¨ä¸Šä¼ çš„æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·ç¡®ä¿æ‚¨å·²ç»ä¸Šä¼ äº†ç›¸å…³æ–‡æ¡£ï¼Œæˆ–è€…å°è¯•ç”¨ä¸åŒçš„æ–¹å¼æè¿°æ‚¨çš„é—®é¢˜ã€‚',
          sources: [],
          conversationId,
        };
      }

      // 2. æ„å»ºä¸Šä¸‹æ–‡
      const context = this.buildContext(relevantChunks);

      // 3. æ„å»º Prompt
      const prompt = this.buildPrompt(message, context);

      // 4. è°ƒç”¨ GPT ç”Ÿæˆå›ç­”
      this.logger.log('ğŸ¤– Generating AI response...');
      const aiResponse = await this.generateAIResponse(prompt);

      // 5. ä¿å­˜å¯¹è¯è®°å½•
      const finalConversationId = await this.saveConversation(
        conversationId,
        message,
        aiResponse,
        relevantChunks
      );

      this.logger.log('âœ… Chat response generated successfully');

      return {
        answer: aiResponse,
        sources: relevantChunks,
        conversationId: finalConversationId,
      };
    } catch (error) {
      this.logger.error('Failed to process chat message:', error);
      throw new BadRequestException('Failed to process your message. Please try again.');
    }
  }

  /**
   * æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
   */
  private buildContext(chunks: any[]): string {
    return chunks
      .map((chunk, index) => `[æ–‡æ¡£ç‰‡æ®µ ${index + 1}]\n${chunk.content}`)
      .join('\n\n');
  }

  /**
   * æ„å»º Prompt
   */
  private buildPrompt(question: string, context: string): string {
    return `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åŸºäºä»–ä»¬ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

**è§’è‰²å®šä¹‰**:
- ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšã€å‡†ç¡®å¯é çš„AIæ–‡æ¡£åŠ©æ‰‹
- ä½ åªåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œä¸æ·»åŠ é¢å¤–ä¿¡æ¯
- ä½ ä¼šä»¥æ¸…æ™°ã€ä¸“ä¸šä¸”æœ‰å¸®åŠ©çš„æ–¹å¼å›ç­”é—®é¢˜

**å›ç­”æŒ‡ä»¤**:
1. ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£ç‰‡æ®µ
2. åªåŸºäºè¿™äº›æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜
3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
4. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ä¸”æœ‰ç”¨
5. ä½¿ç”¨ Markdown æ ¼å¼è®©å›ç­”æ›´æ˜“è¯»
6. å¿…è¦æ—¶å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µ

**æ–‡æ¡£å†…å®¹**:
${context}

**ç”¨æˆ·é—®é¢˜**: ${question}

**å›ç­”è¦æ±‚**:
- ç”¨ä¸­æ–‡å›ç­”
- åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”
- ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡é¢˜å’Œåˆ—è¡¨
- å¦‚æœæ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¯·åˆ†ç‚¹è¯´æ˜

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š`;
  }

  /**
   * è°ƒç”¨ OpenAI API ç”Ÿæˆå›ç­”
   */
  private async generateAIResponse(prompt: string): Promise<string> {
    try {
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–‡æ¡£åŠ©æ‰‹ï¼ŒåŸºäºç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
        max_tokens: 1000,
        temperature: 0.1, // é™ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„å›ç­”
        top_p: 0.9,
      });

      return completion.choices[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚';
    } catch (error) {
      this.logger.error('OpenAI API error:', error);
      throw new Error('Failed to generate AI response');
    }
  }

  /**
   * ä¿å­˜å¯¹è¯è®°å½•
   */
  private async saveConversation(
    conversationId: string | undefined,
    userMessage: string,
    aiResponse: string,
    sources: any[]
  ): Promise<string> {
    try {
      let conversation;

      if (conversationId) {
        // æŸ¥æ‰¾ç°æœ‰å¯¹è¯
        conversation = await this.prisma.conversation.findUnique({
          where: { id: conversationId }
        });
      }

      if (!conversation) {
        // åˆ›å»ºæ–°å¯¹è¯
        conversation = await this.prisma.conversation.create({
          data: {}
        });
      }

      // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
      await this.prisma.chatMessage.create({
        data: {
          conversationId: conversation.id,
          role: 'user',
          content: userMessage,
        }
      });

      // ä¿å­˜AIå›ç­”
      await this.prisma.chatMessage.create({
        data: {
          conversationId: conversation.id,
          role: 'assistant',
          content: aiResponse,
          sources: sources.map(source => ({
            id: source.id,
            documentId: source.documentId,
            chunkIndex: source.chunkIndex,
            similarity: source.similarity,
          })),
        }
      });

      return conversation.id;
    } catch (error) {
      this.logger.error('Failed to save conversation:', error);
      // ä¸æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºè¿™ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
      return conversationId || 'temp-conversation';
    }
  }

  /**
   * è·å–å¯¹è¯å†å²
   */
  async getConversation(conversationId: string) {
    const messages = await this.prisma.chatMessage.findMany({
      where: { conversationId },
      orderBy: { createdAt: 'asc' }
    });

    return messages.map(msg => ({
      id: msg.id,
      role: msg.role,
      content: msg.content,
      sources: msg.sources,
      timestamp: msg.createdAt,
    }));
  }

  /**
   * è·å–æ‰€æœ‰å¯¹è¯
   */
  async getConversations() {
    return await this.prisma.conversation.findMany({
      include: {
        messages: {
          take: 1,
          orderBy: { createdAt: 'desc' }
        }
      },
      orderBy: { updatedAt: 'desc' }
    });
  }
}